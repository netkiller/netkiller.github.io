<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>11.4. YOLO</title><link rel="stylesheet" type="text/css" href="../docbook.css" /><meta name="generator" content="DocBook XSL Stylesheets Vsnapshot" /><meta name="keywords" content="php,pear,pecl,phar, python, , " /><link rel="home" href="../index.html" title="Netkiller Python 手札" /><link rel="up" href="index.html" title="第 11 章 Ultralytics" /><link rel="prev" href="ultralytics.task.html" title="11.3. Task 任务" /><link rel="next" href="ch11s05.html" title="11.5. ONNX(Open Neural Network Exchange) 开放神经网络交换格式" /></head><body><a xmlns="" href="//www.netkiller.cn/">Home</a> | <a xmlns="" href="//netkiller.github.io/">简体中文</a> | <a xmlns="" href="http://netkiller.sourceforge.net/">繁体中文</a> | <a xmlns="" href="/journal/index.html">杂文</a>
		| <a xmlns="" href="https://github.com/netkiller">Github</a> | <a xmlns="" href="https://zhuanlan.zhihu.com/netkiller">知乎专栏</a> | <a xmlns="" href="https://www.facebook.com/bg7nyt">Facebook</a> | <a xmlns="" href="http://cn.linkedin.com/in/netkiller/">Linkedin</a> | <a xmlns="" href="https://www.youtube.com/user/bg7nyt/videos">Youtube</a> | <a xmlns="" href="//www.netkiller.cn/home/donations.html">打赏(Donations)</a> | <a xmlns="" href="//www.netkiller.cn/home/about.html">About</a><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">11.4. YOLO</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="ultralytics.task.html">上一页</a> </td><th width="60%" align="center">第 11 章 Ultralytics</th><td width="20%" align="right"> <a accesskey="n" href="ch11s05.html">下一页</a></td></tr></table><hr /></div><table xmlns=""><tr><td><iframe src="//ghbtns.com/github-btn.html?user=netkiller&amp;repo=netkiller.github.io&amp;type=watch&amp;count=true&amp;size=large" height="30" width="170" frameborder="0" scrolling="0" style="width:170px; height: 30px;" allowTransparency="true"></iframe></td><td><iframe src="//ghbtns.com/github-btn.html?user=netkiller&amp;repo=netkiller.github.io&amp;type=fork&amp;count=true&amp;size=large" height="30" width="170" frameborder="0" scrolling="0" style="width:170px; height: 30px;" allowTransparency="true"></iframe></td><td><iframe src="//ghbtns.com/github-btn.html?user=netkiller&amp;type=follow&amp;count=true&amp;size=large" height="30" width="240" frameborder="0" scrolling="0" style="width:240px; height: 30px;" allowTransparency="true"></iframe></td><td></td><td><a href="https://zhuanlan.zhihu.com/netkiller"><img src="/images/logo/zhihu-card-default.svg" height="25" /></a></td><td valign="middle"><a href="https://zhuanlan.zhihu.com/netkiller">知乎专栏</a></td><td></td><td></td><td></td><td></td></tr></table><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="yolo"></a>11.4. YOLO</h2></div></div></div>
	
	<div class="orderedlist"><p class="title"><strong>目标检测任务</strong></p><ol class="orderedlist" type="1"><li class="listitem">YOLO11：用于经典的目标检测任务。</li><li class="listitem">YOLO11-seg：用于实例分割，识别和分割图像中的对象。</li><li class="listitem">YOLO11-pose：用于关键点姿态估计，即确定人体的关键点（如关节位置）。</li><li class="listitem">YOLO11-obb：用于定向检测，可以识别并确定具有方向性物体的边界框（例如倾斜的目标物体）。</li><li class="listitem">YOLO11-cls：用于分类，负责对图像中的对象进行类别识别。</li></ol></div>
	<div class="orderedlist"><p class="title"><strong>模型规格</strong></p><ol class="orderedlist" type="1"><li class="listitem">YOLOv8 Nano 非常小 YOLOv8n</li><li class="listitem">YOLOv8 Small 小 YOLOv8s</li><li class="listitem">YOLOv8 Medium 中 YOLOv8m</li><li class="listitem">YOLOv8 Large 大 YOLOv8l</li><li class="listitem">YOLOv8 X（Extra Large） 非常大 YOLOv8x</li></ol></div>

	<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id1077"></a>11.4.1. labels 文件格式</h3></div></div></div>
		
		<p>x,y是目标的中心坐标，width,height是目标的宽和高。</p>
		<pre class="screen">
		
&lt;object-class&gt; &lt;x&gt; &lt;y&gt; &lt;width&gt; &lt;height&gt;		
		
		</pre>
		<p>VOC与YOLO之间的格式转换</p>
		<pre class="programlisting">
		
# size是原图的（w,h), box是坐标

def voc_to_yolo(size, box):
    dw = 1./size[0]
    dh = 1./size[1]
    x = (box[0] + box[1])/2.0
    y = (box[2] + box[3])/2.0
    w = box[1] - box[0]
    h = box[3] - box[2]
    x = x*dw
    w = w*dw
    y = y*dh
    h = h*dh
    return (x,y,w,h)
 
def yolo_to_voc(size, box):
    x = box[0] * size[0]
    w = box[2] * size[0]
    y = box[1] * size[1]
    h = box[3] * size[1]
    xmin = int(x - w/2)
    xmax = int(x + w/2)
    ymin = int(y - h/2)
    ymax = int(y + h/2)
    return (xmin, ymin, xmax, ymax)
		
		
		</pre>
	</div>

	<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="yolo.train"></a>11.4.2. 模型训练</h3></div></div></div>
		
		<pre class="programlisting">
		
from ultralytics import YOLO
# 指定模型
mode = YOLO('yolo11n-seg.pt')
# 让模型开始训练
mode.train(data='dataset.yaml',workers=0,epochs=10,batch=10)
# 验证模型
mode.val()		
		
		</pre>
		<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id1078"></a>11.4.2.1. 训练参数</h4></div></div></div>
			
			<p>训练设置所有参数详解</p>
			<p>YOLO 模型的训练设置包括在训练过程中使用的各种超参数和配置。这些设置会影响模型的性能、速度和准确性。关键的训练设置包括批量大小、学习率、动量和权重衰减。此外，优化器的选择、损失函数和训练数据集的组成也会影响训练过程。仔细调整和实验这些设置对于优化性能至关重要。</p>
			<pre class="screen">
			
参数				默认值	描述
model			None	指定训练模型文件。接受 .pt 预训练模型或 .yaml 配置文件的路径。对定义模型结构或初始化权重至关重要。
data			None	数据集配置文件的路径（如 coco8.yaml）。该文件包含数据集特定参数，包括训练和验证数据的路径、类别名称和类别数量。注意：该yaml文件中必须至少包含训练集和验证集数据路径。
epochs			100		总训练 epochs 数。每个 epoch 代表完整遍历整个数据集一次。调整此值会影响训练时长和模型性能。
time			None	最大训练时间（小时）。如果设置，将覆盖 epochs 参数，允许在指定时间后自动停止训练。适用于时间受限的训练场景。
patience		100		在验证指标没有改进的情况下等待的 epochs 数，之后提前停止训练。通过在性能停滞时停止训练来防止过拟合。
batch			16		批量大小，有三种模式：设置为整数（如 batch=16），自动模式为 GPU 内存利用率的 60%（batch=-1），或指定利用率分数的自动模式（batch=0.70）。当你不会根据图像大小计算所需内存时，后面两种模式很有用。
imgsz			640		训练的目标图像大小。所有图像在输入模型前都将调整为此尺寸。影响模型准确性和计算复杂性。
save			True	启用训练检查点和最终模型权重的保存。对于恢复训练或模型部署非常有用。
save_period		-1		模型检查点保存频率，以 epochs 为单位。值为 -1 禁用此功能。在长时间训练期间保存中间模型非常有用。
cache			False	启用数据集图像缓存到内存（True/ram）、磁盘（disk）或禁用（False）。通过减少磁盘 I/O 提高训练速度，但增加内存使用。
device			None	指定训练的计算设备：单个 GPU（device=0），多个 GPU（device=0,1），CPU（device=cpu），或 Apple 芯片的 MPS（device=mps）。
workers			8		数据加载的工作线程数（如果是多 GPU 训练，则按 RANK 计算）。影响数据预处理和输入模型的速度，尤其在多 GPU 设置中。
project			None	保存训练输出的项目目录名称。允许有组织地存储不同实验。
name			None	训练运行的名称。用于在项目文件夹内创建子目录，保存训练日志和输出。
exist_ok		False	如果为 True，允许覆盖现有的项目/名称目录。适用于无需手动清理先前输出的迭代实验。
pretrained		True	决定是否从预训练模型开始训练。可以是布尔值或字符串路径，指定从哪个模型加载权重。增强训练效率和模型性能。
optimizer		'auto'	训练的优化器选择。选项包括 SGD、Adam、AdamW、NAdam、RAdam、RMSProp 等，或根据模型配置自动选择。影响收敛速度和稳定性。
verbose			False	启用详细输出，在训练期间提供详细日志和进度更新。适用于调试和密切监控训练过程。
seed			0		设置训练的随机种子，确保在相同配置下结果的可重复性。
deterministic	True	强制使用确定性算法，确保可重复性，但可能因限制非确定性算法而影响性能和速度。
single_cls		False	在训练期间将多类数据集中的所有类视为单一类。适用于二分类任务或专注于对象存在而非分类时。
rect			False	启用矩形训练，优化批次组成以最小化填充。可以提高效率和速度，但可能影响模型准确性。
cos_lr			False	使用余弦学习率调度器，按余弦曲线调整学习率。帮助更好地管理学习率以实现更好的收敛。
close_mosaic	10		在最后 N 个 epochs 中禁用马赛克数据增强，以在完成前稳定训练。设置为 0 禁用此功能。
resume			False	从上次保存的检查点恢复训练。自动加载模型权重、优化器状态和 epoch 计数，继续无缝训练。
amp				True	启用自动混合精度（AMP）训练，减少内存使用并可能加快训练速度，对准确性的影响最小。
fraction		1.0		指定用于训练的数据集的比例。允许在数据集的子集中进行训练，适用于实验或资源有限时。
profile			False	启用训练期间 ONNX 和 TensorRT 速度的分析，优化模型部署。
freeze			None	冻结模型的前 N 层或按索引指定的层，减少可训练参数数量。适用于微调或迁移学习。
lr0				0.01	初始学习率（如 SGD=1E-2，Adam=1E-3）。调整此值对优化过程至关重要，影响模型权重的更新速度。
lrf				0.01	最终学习率作为初始率的分数 = (lr0 * lrf)，与调度器一起使用以随时间调整学习率。
momentum		0.937	SGD 的动量因子或 Adam 的 beta1，影响当前更新中过去梯度的合并。
weight_decay	0.0005	L2 正则化项，惩罚大权重以防止过拟合。
warmup_epochs	3.0		学习率预热的 epochs 数，从低值逐渐增加到初始学习率，以在早期稳定训练。
warmup_momentum	0.8		预热阶段的初始动量，在预热期内逐渐调整到设定动量。
warmup_bias_lr	0.1		预热阶段偏置参数的学习率，有助于在初始 epochs 中稳定模型训练。
box				7.5		损失函数中 box 损失组件的权重，影响准确预测边界框坐标的重要性。
cls				0.5		总损失函数中分类损失的权重，影响正确类别预测相对于其他组件的重要性。
dfl				1.5		分布焦点损失的权重，用于某些 YOLO 版本的细粒度分类。
pose			12.0	在姿态估计训练的模型中姿态损失的权重，影响准确预测姿态关键点的重要性。
kobj			2.0		姿态估计模型中关键点对象性的损失权重，平衡检测置信度与姿态准确性。
label_smoothing	0.0		应用标签平滑，将硬标签软化为目标标签和标签间的均匀分布的混合，可以改善泛化。
nbs				64		用于损失归一化的名义批大小。
overlap_mask	True	决定在训练期间分割掩码是否应重叠，适用于实例分割任务。
mask_ratio		4		分割掩码的下采样率，影响训练期间使用的掩码分辨率。
dropout			0.0		分类任务的正则化丢弃率，通过在训练期间随机忽略单元来防止过拟合。
val				True	启用训练期间的验证，允许定期评估模型在单独数据集上的性能。
plots			False	生成并保存训练和验证指标的图表以及预测示例，为模型性能和学习进展提供视觉洞察。
			
			</pre>
			
		</div>
		<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id1079"></a>11.4.2.2. 图像增强设置</h4></div></div></div>
			
			<pre class="screen">
			
参数		类型		默认值	范围				描述
hsv_h	float	0.015	0.0 - 1.0		调整图像的色调，增加颜色变化，有助于模型在不同光照条件下泛化。
hsv_s	float	0.7		0.0 - 1.0		改变图像的饱和度，影响颜色的强度。用于模拟不同的环境条件。
hsv_v	float	0.4		0.0 - 1.0		修改图像的亮度，有助于模型在各种光照条件下表现良好。
degrees	float	0.0		-180 - +180		随机旋转图像，提升模型识别不同方向物体的能力。
translate	float	0.1	0.0 - 1.0		水平和垂直平移图像，帮助模型学会检测部分可见的物体。
scale	float	0.5		&gt;=0.0			按比例缩放图像，模拟不同距离的物体。
shear	float	0.0		-180 - +180		按指定角度剪切图像，模拟从不同角度观看物体的效果。
perspective	float	0.0	0.0 - 0.001		对图像应用随机透视变换，增强模型理解三维空间中物体的能力。
flipud	float	0.0		0.0 - 1.0		以指定概率上下翻转图像，增加数据多样性而不影响物体特性。
fliplr	float	0.5		0.0 - 1.0		以指定概率左右翻转图像，有助于学习对称物体，并增加数据集多样性。
bgr		float	0.0		0.0 - 1.0		以指定概率将图像通道从 RGB 翻转为 BGR，提高对错误通道顺序的鲁棒性。
mosaic	float	1.0		0.0 - 1.0		将四张训练图像组合成一张，模拟不同场景构图和物体交互。非常有效的复杂场景理解。
mixup	float	0.0		0.0 - 1.0		将两张图像及其标签混合，创建复合图像。通过引入标签噪声和视觉变化，提高模型的泛化能力。
copy_paste	float	0.0	0.0 - 1.0		将一个图像中的物体复制并粘贴到另一个图像中，有助于增加物体实例并学习物体遮挡。
auto_augment	str		randaugment	-	自动应用预定义的增强策略（randaugment、autoaugment、augmix），通过多样化视觉特征优化分类任务。
erasing	float	0.4		0.0 - 0.9		在分类训练期间随机擦除图像的一部分，鼓励模型关注较不明显的特征。
crop_fraction	float	1.0	0.1 - 1.0	将分类图像裁剪为其大小的一部分，以强调中心特征并适应物体尺度，减少背景干扰。
			
			</pre>
			<p>用户可以根据自身的需求，设置数据集和任务的具体参数。通过不同值的调整，找到最佳的策略，从而达到最佳的模型训练效果。</p>
		</div>
		<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id1080"></a>11.4.2.3. </h4></div></div></div>
			
			<pre class="screen">
			
			
			
			</pre>
			<pre class="programlisting">
			
			
			
			</pre>
		</div>
		<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id1081"></a>11.4.2.4. 输出结果</h4></div></div></div>
			
			<pre class="screen">
			
      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
     64/100     0.679G     0.5564     0.5321      1.084          9        320: 100%|██████████| 231/231 [00:25&lt;00:00,  8.97it/s]
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 34/34 [00:05&lt;00:00,  6.49it/s]
                   all       1068       1068      0.958      0.943      0.984      0.944			
			
			</pre>
			<p>Epoch GPU_mem box_loss cls_loss dfl_loss Instances Size</p>
			<div class="glosslist"><dl><dt><span class="glossterm">Epoch</span></dt><dd class="glossdef"><p>训练过程中的迭代次数，在机器学习和深度学习中，一个epoch是训练过程中的一个完整周期，包括了训练数据集中的所有样本，以逐渐优化其性能。
						</p></dd><dt><span class="glossterm">GPU_mem</span></dt><dd class="glossdef"><p>GPU显存的使用情况，也就是图形处理器在训练过程中所使用的显存量。</p></dd><dt><span class="glossterm">box_loss</span></dt><dd class="glossdef"></dd><dt><span class="glossterm">cls_loss</span></dt><dd class="glossdef"></dd><dt><span class="glossterm">dfl_loss</span></dt><dd class="glossdef"></dd><dt><span class="glossterm">Instances</span></dt><dd class="glossdef"></dd><dt><span class="glossterm">Size</span></dt><dd class="glossdef"></dd></dl></div>
			<p>Class Images Instances Box(P R mAP50 mAP50-95):</p>
			<div class="glosslist"><p class="title"><strong></strong></p><dl><dt><span class="glossterm">Class</span></dt><dd class="glossdef"></dd><dt><span class="glossterm">Images</span></dt><dd class="glossdef"></dd><dt><span class="glossterm">Instances</span></dt><dd class="glossdef"></dd><dt><span class="glossterm">P</span></dt><dd class="glossdef"></dd><dt><span class="glossterm">R</span></dt><dd class="glossdef"></dd><dt><span class="glossterm">mAP50</span></dt><dd class="glossdef"></dd><dt><span class="glossterm">mAP50-95</span></dt><dd class="glossdef"></dd></dl></div>

		</div>
		<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id1090"></a>11.4.2.5. 训练结果</h4></div></div></div>
			
			<div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="id1082"></a>weights 文件夹</h5></div></div></div>
				
				<p>打开 weights 文件夹里面有两个文件，分别是：best.pt和last.pt。</p>
				<p>best.pt 是整个训练过程中，性能最好的模型权重文件。</p>
				<p>last.pt 是最后一次训练的模型权重文件。</p>
				<p>一般我们选择 best.pt 文件，用于生产环境。</p>
			</div>
			<div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="id1083"></a>args.yaml 文件</h5></div></div></div>
				
				<pre class="screen">
				
task: detect
mode: train
model: tongue.yaml
data: datasets/test/YOLODataset/dataset.yaml
epochs: 100
time: null
patience: 100
batch: 16
imgsz: 320
save: true
save_period: -1
cache: false
device: 0
workers: 8
project: null
name: train9
exist_ok: false
pretrained: true
optimizer: auto
verbose: true
seed: 0
deterministic: true
single_cls: false
rect: false
cos_lr: false
close_mosaic: 10
resume: false
amp: true
fraction: 1.0
profile: false
freeze: null
multi_scale: false
overlap_mask: true
mask_ratio: 4
dropout: 0.0
val: true
split: val
save_json: false
save_hybrid: false
conf: null
iou: 0.7
max_det: 300
half: false
dnn: false
plots: true
source: null
vid_stride: 1
stream_buffer: false
visualize: false
augment: false
agnostic_nms: false
classes: null
retina_masks: false
embed: null
show: false
save_frames: false
save_txt: false
save_conf: false
save_crop: false
show_labels: true
show_conf: true
show_boxes: true
line_width: null
format: torchscript
keras: false
optimize: false
int8: false
dynamic: false
simplify: true
opset: null
workspace: null
nms: false
lr0: 0.01
lrf: 0.01
momentum: 0.937
weight_decay: 0.0005
warmup_epochs: 3.0
warmup_momentum: 0.8
warmup_bias_lr: 0.1
box: 7.5
cls: 0.5
dfl: 1.5
pose: 12.0
kobj: 1.0
nbs: 64
hsv_h: 0.015
hsv_s: 0.7
hsv_v: 0.4
degrees: 0.0
translate: 0.1
scale: 0.5
shear: 0.0
perspective: 0.0
flipud: 0.0
fliplr: 0.5
bgr: 0.0
mosaic: 1.0
mixup: 0.0
copy_paste: 0.0
copy_paste_mode: flip
auto_augment: randaugment
erasing: 0.4
crop_fraction: 1.0
cfg: null
tracker: botsort.yaml
save_dir: D:\workspace\netkiller\runs\detect\train
				
				</pre>
			</div>
			<div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="id1084"></a>confusion_matrix.png / confusion_matrix_normalized.png
				</h5></div></div></div>
				
				<p>混淆矩阵的归一化版本</p>
			</div>

			<div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="id1085"></a>P_curve.png / R_curve.png/ PR_curve.png /F1_curve.png</h5></div></div></div>
				
				<div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">P_curve.png 每个 label 的精度</li><li class="listitem">R_curve.png 每个 label 的召回率</li><li class="listitem">PR_curve.png 精度与召回互相妥协的曲线。范围 0.0 ~ 1.0 越接近 1.0 效果越好。
					</li></ol></div>
			</div>

			<div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="id1086"></a>labels.jpg / labels_correlogram.jpg</h5></div></div></div>
				
				<p>labels.jpg 分类标签的分布</p>
				<p></p>
			</div>

			<div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="id1087"></a>results.png / results.csv</h5></div></div></div>
				
				<p>先看前三列是 loss
					损失率图表，X轴表示训练轮次，Y轴表示损失值。推理结果和人工标注的答案之间的差异，称为“损失”。损失越小越好，损失为0则说明推理和正确答案之间没有差异，即预测100%命中。
				</p>
				<div class="orderedlist"><p class="title"><strong>train 训练集，val 验证集</strong></p><ol class="orderedlist" type="1"><li class="listitem">train/box_loss和val/box_loss</li><li class="listitem">train/cls_loss和val/cls_loss</li><li class="listitem">train/dfl_loss和val/dfl_loss</li></ol></div>
				<div class="orderedlist"><p class="title"><strong>精度/召回率</strong></p><ol class="orderedlist" type="1"><li class="listitem">metric/precision(B) 精度</li><li class="listitem">metrics/recall(B) 召回率</li></ol></div>
				<p>mAP50是重合度以50%为界限的平均精度。而mAP50-95则是IoU阈值从50%到95%范围内的平均值</p>
				<p>X轴是训练轮次，Y表示精度，取值0.0 ~ 1.0（100%）越接近1.0效果越好。</p>
			</div>
			<div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="id1088"></a>train_batch0.jpg</h5></div></div></div>
				
			</div>
			<div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="id1089"></a>val_batch0_labels.jpg / val_batch0_pred.jpg</h5></div></div></div>
				
				<p>评估对照组，对比两个图片，可以看到训练结果，标签是否都正确预测。</p>
			</div>
		</div>
		<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id1091"></a>11.4.2.6. 恢复中断的训练</h4></div></div></div>
			
			<p>如果训练过程异常中断，可以通过设置 resume=True 选项，读取训练过程，断点信息存储在 ‘path/to/last.pt’ 文件中，启动后会从之前的断点恢复训练，</p>
			<pre class="programlisting">
			
from ultralytics import YOLO

# 加载模型
model = YOLO("path/to/last.pt")  # 加载训练的模型

# 恢复训练
results = model.train(resume=True)
			
			
			</pre>

		</div>

	</div>
	<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="yolo.val"></a>11.4.3. 模型评估</h3></div></div></div>
		
		<pre class="programlisting">
		
from ultralytics import YOLO
# ---------- 加载模型 ----------
model = YOLO('runs/detect/train/weights/best.pt')
# ---------- 模型评估 ----------
model.val(data='captcha/images/YOLODataset/dataset.yaml')
		
		
		</pre>
		<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id1092"></a>11.4.3.1. 选择数据集</h4></div></div></div>
			
			<p>split='test' 可以选择数据集，默认是 val</p>
			<pre class="programlisting">
			
from ultralytics import YOLO
 
# Load a model
model = YOLO('yolov11n.pt')  # load an official model
model = YOLO('runs/detect/train/weights/best.pt')  # load a custom model
 
# Validate the model
metrics = model.val(split='test')  # no arguments needed, dataset and settings remembered
metrics.box.map    # map50-95
metrics.box.map50  # map50
metrics.box.map75  # map75
metrics.box.maps   # a list contains map50-95 of each category			
			
			</pre>
		</div>
		<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id1093"></a>11.4.3.2. 验证参数设置</h4></div></div></div>
			
			<pre class="screen">
			
参数	类型	默认值	描述
data		str		None	指定数据集配置文件的路径（例如，coco8.yaml）。这个文件包括验证数据的路径、类名和类的数量。
imgsz		int		640		定义输入图像的大小。所有图像在处理前都调整到这个尺寸。
batch		int		16		设置每个批次的图像数量。使用 -1 表示自动批量（AutoBatch），它会根据 GPU 内存可用性自动调整。
save_json	bool	False	如果为 True，将结果保存到 JSON 文件中，以便进一步分析或与其他工具集成。
save_hybrid	bool	False	如果为 True，保存标签的混合版本，将原始注释与额外的模型预测相结合。
conf		float	0.001	设置检测的最小置信阈值。丢弃置信度低于这个阈值的检测。
iou			float	0.6		设置非极大值抑制（NMS）的交并比（IoU）阈值。有助于减少重复检测。
max_det		int		300		限制每张图像的最大检测数量。在密集场景中用于防止过度检测。
half		bool	True	启用半精度（FP16）计算，减少内存使用，并可能在最小影响准确性的情况下提高速度。
device		str		None	指定验证的设备（cpu, cuda:0 等）。允许灵活使用 CPU 或 GPU 资源。
dnn			bool	False	如果为 True，使用 OpenCV DNN 模块进行 ONNX 模型推理，作为 PyTorch 推理方法的替代。
plots		bool	False	当设置为 True 时，生成并保存预测与真实值的对比图，以便直观评估模型的性能。
rect		bool	False	如果为 True，使用矩形推理进行批处理，减少填充，可能提高速度和效率。
split		str		val		确定用于验证的数据集划分（val, test, 或 train）。允许灵活选择用于性能评估的数据段。			
			
			</pre>
		</div>
	</div>
	<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="model.predict"></a>11.4.4. 模型预测</h3></div></div></div>
		
		<pre class="programlisting">
		
from ultralytics import YOLO

# 加载预训练的YOLOv11n模型
model = YOLO("yolo11n.pt")  # yolo11n.pt, yolo11s.pt, yolo11m.pt, yolo11x.pt

# 对'bus.jpg'图像进行推理，并获取结果 https://ultralytics.com/images/bus.jpg
results = model.predict("https://ultralytics.com/images/bus.jpg", save=True, imgsz=320, conf=0.5)

# 处理返回的结果
for result in results:
    boxes = result.boxes  # 获取边界框信息
    probs = result.probs  # 获取分类概率
    result.show()  # 显示结果型的架构
    print(boxes)
    print(probs)		
		
		</pre>
		<p>换一个模型，试试 yolo11n-seg.pt</p>
		<pre class="programlisting">
		
from ultralytics import YOLO

model = YOLO("yolo11n-seg.pt")
results = model.predict("https://ultralytics.com/images/bus.jpg", save=True, imgsz=320, conf=0.5)

# 处理返回的结果
for result in results:
    boxes = result.boxes  # 获取边界框信息
    probs = result.probs  # 获取分类概率
    print(boxes)
    print(probs)
    result.show()  # 显示结果型的架构		
		
		</pre>

		<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id1094"></a>11.4.4.1. predict 参数</h4></div></div></div>
			
			<p>推理参数</p>
			<pre class="screen">
			
参数				类型			默认值	描述
source			str			‘ultralytics/assets’	指定推理的数据源。可以是图像路径、视频文件、目录、URL 或实时流的设备 ID。支持多种格式和来源，使不同类型的输入都能灵活应用。
conf			float		0.25	设置检测的最小置信度阈值。低于此阈值的检测对象将被忽略。调整此值有助于减少误报。
iou				float		0.7		非极大值抑制（NMS）的交并比（IoU）阈值。较低的值会通过消除重叠的框来减少检测数量，对于减少重复检测很有用。
imgsz			int/tuple	640		定义推理的图像大小。可以是一个整数 640，用于方形缩放，也可以是一个（高度，宽度）元组。适当的尺寸可以提高检测准确性和处理速度。
half			bool		False	启用半精度（FP16）推理，可以在支持 GPU 上加速模型推理，同时对准确度的影响最小。
device			str			None	指定推理的设备（例如，cpu, cuda:0 或 0）。允许用户在 CPU、特定 GPU 或其他计算设备之间选择模型执行。
max_det			int			300		每张图像允许的最大检测数量。限制模型在单个推理中可以检测到的总对象数量，以防止在密集场景中产生过多的输出。
vid_stride		int			1		视频输入的帧间跳过。允许在视频中跳过帧以提高处理速度，但代价是时间分辨率。值为 1 时处理每个帧，更高的值跳过帧。
stream_buffer	bool		False	确定在处理视频流时是否应缓冲所有帧（True），或者模型是否应返回最新的帧（False）。对于实时应用很有用。
visualize		bool		False	激活推理期间模型特征的可视化，提供模型“看到”的洞察。对于调试和模型解释非常有用。
augment			bool		False	启用预测的测试时间增强（TTA），可能会提高检测的鲁棒性，但代价是推理速度。
agnostic_nms	bool		False	启用类无关的非极大值抑制（NMS），合并不同类的重叠框。在多类检测场景中，当类之间经常有重叠时非常有用。
classes			list [int]	None	过滤预测到一组类 ID。只有属于指定类的检测结果才会被返回。对于在多类检测任务中关注相关对象非常有用。
retina_masks	bool		False	如果模型中可用，则使用高分辨率的分割掩码。这可以增强分割任务的掩码质量，提供更细的细节。
embed			list [int]	None	指定要从哪些层提取特征向量或嵌入。对于下游任务，如聚类或相似度搜索非常有用。			
			
			</pre>
			<p>可视化参数</p>
			<pre class="screen">
			
参数			类型		默认值	描述
show		bool	False	如果为 True，则在窗口中显示标注的图像或视频。在开发或测试期间，这对于立即获得视觉反馈非常有用。
save		bool	False	启用将标注的图像或视频保存到文件。这对于文档、进一步分析或分享结果非常有用。
save_frames	bool	False	在处理视频时，将单独的帧保存为图像。这对于提取特定帧或进行详细的逐帧分析非常有用。
save_txt	bool	False	将检测结果保存为文本文件，格式为 [class] [x_center] [y_center] [width] [height] [confidence]。这对于与其他分析工具集成非常有用。
save_conf	bool	False	在保存的文本文件中包含置信度分数。增强了可用于后处理和分析的详细信息。
save_crop	bool	False	保存检测的裁剪图像。这对于数据增强、分析或创建特定对象的聚焦数据集非常有用。
show_labels	bool	True	在视觉输出中显示每个检测的标签。提供了对检测到的对象立即的理解。
show_conf	bool	True	在标签旁边显示每个检测的置信度分数。为每个检测提供了模型确定性的洞察。
show_boxes	bool	True	在图像或视频帧中检测到的对象周围绘制边界框。这对于视觉识别和定位图像或视频帧中的对象至关重要。
line_width	int		None	指定边界框的线条宽度。如果为 None，线条宽度将根据图像大小自动调整。为清晰度提供了视觉定制。
			
			
			</pre>
			<pre class="programlisting">
			
from yolov8 import YOLO
 
# 加载预训练模型
model = YOLO('yolov8.pt')
 
# 执行预测
results = model.predict(
    source='path/to/image.jpg',  # 输入源，可以是图像路径、视频路径、摄像头索引等
    conf=0.5,                    # 置信度阈值，滤除低于该置信度的检测结果。
    iou=0.45,                    # NMS的交并比阈值，用于消除重叠的检测框。
    device='cuda',               # 使用GPU
    imgsz=(640, 640),            # 输入图像的尺寸，可以是单个整数（长宽相等）或元组（长和宽）。
    half=False,                  # 不使用半精度浮点数
    save_txt=True,               # 保存预测结果为文本文件
    save_conf=True,              # 在保存的文本文件中包含置信度
    save_crop=False,             # 不保存裁剪后的检测结果
    save_img=True,               # 保存带有检测框的图像
    classes=[0, 1, 2],           # 仅检测指定的类
    agnostic_nms=False,          # 不使用类别无关的NMS
    augment=False,               # 不使用数据增强
    visualize=False,             # 不可视化特征图
    project='runs/detect',       # 保存预测结果的项目目录
    name='exp',                  # 保存预测结果的子目录名称
    exist_ok=False,              # 不允许项目目录存在
    line_thickness=3,            # 绘制检测框的线条粗细
    hide_labels=False,           # 显示检测框的标签
    hide_conf=False              # 显示检测框的置信度
)
 
# 输出预测结果
print(results)
			
			
			</pre>
		</div>
		<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id1095"></a>11.4.4.2. 返回结果</h4></div></div></div>
			
			<p>返回结果通常包含以下主要属性和方法：</p>
			<div class="literallayout"><p><br />
			<br />
1.boxes: 包含检测到的边界框信息的对象。<br />
2.scores: 包含每个检测目标的置信度分数。<br />
3.classes: 包含每个检测目标的类别索引。<br />
4.masks: 如果使用实例分割模型，包含分割掩码。<br />
5.keypoints: 如果使用关键点检测模型，包含关键点信息。<br />
6.orig_img: 原始输入图像。<br />
7.plot(): 用于可视化检测结果的方法。			<br />
			<br />
			</p></div>
			<p>boxes: </p>
			<div class="literallayout"><p><br />
			<br />
1.boxes.xyxy: 返回每个检测目标的边界框坐标，格式为 [x_min, y_min, x_max, y_max]。<br />
2.boxes.conf: 返回每个检测目标的置信度分数。<br />
3.boxes.cls: 返回每个检测目标的类别索引。			<br />
			<br />
			</p></div>
		</div>
		<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id1096"></a>11.4.4.3. 提取标签</h4></div></div></div>
			
			<pre class="programlisting">
			
from ultralytics import YOLO

# 加载模型
# model = YOLO("yolo11n.pt")
# model = YOLO('weights/last.pt')
model = YOLO('weights/best.pt')

# 模型预测，save=True 的时候表示直接保存yolov8的预测结果
results = model('images/0_5902.png')

# View results
for result in results:
    # 处理结果
    boxes = result.boxes  # 边界框结果
    masks = result.masks  # 分割掩码结果
    names = result.names
    keypoints = result.keypoints  # 关键点检测结果
    probs = result.probs  # 分类概率结果
    obb = result.obb  # 方向边界框结果（OBB）
    print(boxes)
    for box in boxes:
        print(names[int(box.cls)])
    result.show()  # 显示结果			
			
			</pre>
		</div>
		<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id1097"></a>11.4.4.4. 保存识别图像</h4></div></div></div>
			
			<pre class="programlisting">
			
@app.post("/netkiller")
async def netkiller(file: bytes = File()):
    try:
        filename = f"tmp/{uuid.uuid4()}.png"
        print(filename)
        with open(filename, "wb") as f:
            f.write(file)

        results = model(filename) # , conf=0.45

        for result in results:
            boxes = result.boxes  # 获取边界框信息
            # probs = result.probs  # 获取分类概率
            names = result.names
            data = result.boxes.data
            print(boxes)
            # print(probs)
            if names is not None:
                labels = []
                for box in boxes:
                    label = names[int(box.cls)]
                    labels.append(label)
                    print(label)
                # result.show()  # 显示结果型的架构
                result.save(filename="tmp/test.png")

            if boxes is not None:
                image = Image.open(filename)
                # x0, y0, x1, y1 = map(int, boxes.xyxy[0])
                crop = tuple(map(int, boxes.xyxy[0]))

                region = image.crop(crop)
                region.save('tmp/neo.jpg')

            return {"label": labels}
    except Exception as e:
        log.error(e)
    return {"label": None}			
			
			</pre>
		</div>
		<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="result.boxes"></a>11.4.4.5. boxes</h4></div></div></div>
			
			<p>tensor([ 59.0264, 88.7340, 367.5180, 420.6993]) 转 list [ 59.0264, 88.7340, 367.5180, 420.6993]</p>
			<pre class="programlisting">
			
xyxy = boxes.xyxy[0].tolist()			
			
			</pre>
			<p>boxes.data 数据，YOLO 边界框格式：[x1, y1, x2, y2, confidence, class]</p>
			<pre class="programlisting">
			
boxes = result.boxes.data.cpu().numpy()
print(result.boxes.data.tolist())		
			
			</pre>
			<pre class="programlisting">
			
    def boxes(self, source:str,target:str):
        if not os.path.exists(source):
            return None

        try:

            image = cv2.imread(source)
            if image is None:
                return None

            results = self.model(source,verbose=False)

            for result in results:
                # 提取检测到的目标的边界框
                boxes = result.boxes.data.cpu().numpy()  # YOLO 边界框格式：[x1, y1, x2, y2, confidence, class]
                # print(result.boxes.data.tolist())
                if self.args.output:
                    result.save(filename=os.path.join(self.args.output, os.path.basename(source)))
                    result.save_crop(save_dir=os.path.join(self.args.output,'crop'), file_name="detection")
                # print(boxes)

                for idx, box in enumerate(boxes):
                    x1, y1, x2, y2, conf, cls = map(int, box[:6])
                    cropped = image[y1:y2, x1:x2]
                    output = os.path.join(self.args.target, f"{os.path.splitext(os.path.basename(source))[0]}_{idx}.jpg")
                    cv2.imwrite(output, cropped)
                    # print(f"Saved cropped image: {output}")
        except Exception as e:
            # log.error(e)
            print("boxes: ",e)
            exit()
        return None		
			
			</pre>
						
		</div>
		<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id1098"></a>11.4.4.6. verbose 打印详细输出</h4></div></div></div>
			
			<p>默认会输出</p>
			<pre class="screen">
			
image 1/1 /Users/neo/PycharmProjects/netkiller/tmp/test/20150917_082555.jpg: 320x192 1 shebei, 11.2ms
Speed: 0.5ms preprocess, 11.2ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 192)
			
			</pre>
			<p>关闭输出</p>
			<pre class="programlisting">
			
model(img, save=True, verbose=False)			
			
			</pre>
		</div>
	</div>
	<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="model.export"></a>11.4.5. 模型导出</h3></div></div></div>
		
		<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id1099"></a>11.4.5.1. onnx 格式</h4></div></div></div>
			
			<pre class="screen">
			
(.venv) neo@Neo-Mac-mini netkiller % pip install onnx onnxslim -i https://pypi.tuna.tsinghua.edu.cn/simple		
			
			</pre>
			<pre class="programlisting">
			
from ultralytics import YOLO

model = YOLO("yolo11n.pt")
model.export(format="onnx")

# 预测看下效果
onnx_model = YOLO("yolo11n.onnx", task="detect")
results = onnx_model("https://ultralytics.com/images/bus.jpg")
results[0].show()		
			
			</pre>
		</div>
		<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id1100"></a>11.4.5.2. engine 格式</h4></div></div></div>
			
			<pre class="programlisting">
			
from ultralytics import YOLO

model = YOLO("yolo11n.pt")
model.export(format="engine")
tensorrt_model = YOLO("yolo11n.engine", task="detect")
results = tensorrt_model("https://ultralytics.com/images/bus.jpg")
results[0].show()			
			
			</pre>
		</div>

	</div>
</div><script xmlns="" type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?u=r5HG&amp;d=9mi5r_kkDC8uxG8HuY3p4-2qgeeVypAK9vMD-2P6BYM"></script><div class="navfooter"><hr /><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="ultralytics.task.html">上一页</a> </td><td width="20%" align="center"><a accesskey="u" href="index.html">上一级</a></td><td width="40%" align="right"> <a accesskey="n" href="ch11s05.html">下一页</a></td></tr><tr><td width="40%" align="left" valign="top">11.3. Task 任务 </td><td width="20%" align="center"><a accesskey="h" href="../index.html">起始页</a></td><td width="40%" align="right" valign="top"> 11.5. ONNX(Open Neural Network Exchange) 开放神经网络交换格式</td></tr></table></div><script xmlns="">
			(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

			ga('create', 'UA-11694057-1', 'auto');
			ga('send', 'pageview');

		</script><script xmlns="" async="async">
			var _hmt = _hmt || [];
			(function() {
			var hm = document.createElement("script");
			hm.src = "https://hm.baidu.com/hm.js?93967759a51cda79e49bf4e34d0b0f2c";
			var s = document.getElementsByTagName("script")[0];
			s.parentNode.insertBefore(hm, s);
			})();
</script><script xmlns="" async="async">
			(function(){
			var bp = document.createElement('script');
			var curProtocol = window.location.protocol.split(':')[0];
			if (curProtocol === 'https') {
			bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
			}
			else {
			bp.src = 'http://push.zhanzhang.baidu.com/push.js';
			}
			var s = document.getElementsByTagName("script")[0];
			s.parentNode.insertBefore(bp, s);
			})();
</script></body></html>